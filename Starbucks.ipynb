{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Exercise: Starbucks\n",
    "<br>\n",
    "\n",
    "<img src=\"https://opj.ca/wp-content/uploads/2018/02/New-Starbucks-Logo-1200x969.jpg\" width=\"200\" height=\"200\">\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "#### Background Information\n",
    "\n",
    "The dataset in this exercise was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.\n",
    "\n",
    "#### Optimization Strategy\n",
    "\n",
    "The task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, the goal is to maximize the following metrics:\n",
    "\n",
    "* **Incremental Response Rate (IRR)** \n",
    "\n",
    "IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (_treatment_) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (_control_).\n",
    "\n",
    "$$ IRR = \\frac{purch_{treat}}{cust_{treat}} - \\frac{purch_{ctrl}}{cust_{ctrl}} $$\n",
    "\n",
    "\n",
    "* **Net Incremental Revenue (NIR)**\n",
    "\n",
    "NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.\n",
    "\n",
    "$$ NIR = (10\\cdot purch_{treat} - 0.15 \\cdot cust_{treat}) - 10 \\cdot purch_{ctrl}$$\n",
    "\n",
    "For a full description of what Starbucks provides to candidates see the [instructions available here](https://drive.google.com/open?id=18klca9Sef1Rs6q8DW4l7o349r8B70qXM).\n",
    "\n",
    "Below you can find the training data provided.  Explore the data and different optimization strategies.\n",
    "\n",
    "#### How To Test Your Strategy?\n",
    "\n",
    "When you feel like you have an optimization strategy, complete the `promotion_strategy` function to pass to the `test_results` function.  \n",
    "From past data, we know there are four possible outomes:\n",
    "\n",
    "Table of actual promotion vs. predicted promotion customers:  \n",
    "\n",
    "<table>\n",
    "<tr><th></th><th colspan = '2'>Actual</th></tr>\n",
    "<tr><th>Predicted</th><th>Yes</th><th>No</th></tr>\n",
    "<tr><th>Yes</th><td>I</td><td>II</td></tr>\n",
    "<tr><th>No</th><td>III</td><td>IV</td></tr>\n",
    "</table>\n",
    "\n",
    "The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II.  Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.  \n",
    "\n",
    "Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future. \n",
    "\n",
    "Get started by reading in the data below.  See how each variable or combination of variables along with a promotion influences the chance of purchasing.  When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final `test_results` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in packages\n",
    "from itertools import combinations\n",
    "\n",
    "from test_results import test_results, score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load in the data\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by exploring the data, and making sure it is clean for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset has 84534 individuals. Among them, 42364 received the promotion and 42170 did not\n"
     ]
    }
   ],
   "source": [
    "# We visualize the shape of our dataset\n",
    "print('Our training dataset has {} individuals. \\\n",
    "Among them, {} received the promotion and {} did not'.format(len(train_data),\n",
    "                                                            len(train_data.loc[train_data.Promotion == 'Yes']),\n",
    "                                                            len(train_data.loc[train_data.Promotion == 'No'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promotion\n",
       "No     0.007565\n",
       "Yes    0.017019\n",
       "Name: purchase, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We then compare the response rate for those who received the promotion and those who did not\n",
    "train_data.groupby('Promotion')['purchase'].agg(lambda x: x.sum()/x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Promotion    0\n",
       "purchase     0\n",
       "V1           0\n",
       "V2           0\n",
       "V3           0\n",
       "V4           0\n",
       "V5           0\n",
       "V6           0\n",
       "V7           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check for missing values\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check for duplicates\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally we visualize the distribution of values to make sure all our data seems valid\n",
    "train_data.iloc[:,1:].boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is relatively evenly split between people who received the promotion and those who did not, so the assignment between treatment and control appears to have been random. At first glance, the response rate (not taking incrementality into account) for people who received the promotion received a boost, but we will need to assess the significance of this result. \n",
    "\n",
    "We do not have missing values nor duplicates, and our features do not appear to have outliers (except for the feature V2). Given the lack of information about what the feature represents, we do not remove these extreme values, but we could test our approach with these values removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_irr(df):\n",
    "    '''\n",
    "    df (dataframe): a dataframe containing the results for the treatment and control groups\n",
    "    \n",
    "    OUTPUT: a float representing the incremental response rate of this promotional campaign\n",
    "    '''\n",
    "    \n",
    "    purch_treat = df.loc[df.Promotion == 'Yes','purchase'].sum()\n",
    "    purch_cont = df.loc[df.Promotion == 'No','purchase'].sum()\n",
    "    nb_treat = (df.Promotion == 'Yes').sum()\n",
    "    nb_cont = (df.Promotion == 'No').sum()\n",
    "    \n",
    "    irr = (purch_treat/nb_treat) - (purch_cont/nb_cont)\n",
    "    \n",
    "    return irr\n",
    "\n",
    "def calculate_nir(df):\n",
    "    ''''\n",
    "    df (dataframe): a dataframe containing the results for the treatment and control groups\n",
    "    \n",
    "    OUTPUT: a float representing the net incremental revenue of this promotional campaign\n",
    "    '''\n",
    "    purch_treat = df.loc[df.Promotion == 'Yes','purchase'].sum()\n",
    "    purch_cont = df.loc[df.Promotion == 'No','purchase'].sum()\n",
    "    nb_treat = (df.Promotion == 'Yes').sum()\n",
    "    \n",
    "    nir = (10*purch_treat - 0.15*nb_treat) - purch_cont\n",
    "    \n",
    "    return nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incremental response rate for this promotion on the training data is 0.9%, which translates intoa net incremental revenue of $536.40\n"
     ]
    }
   ],
   "source": [
    "print('The incremental response rate for this promotion on the training data is {:.1%}, which translates into\\\n",
    "a net incremental revenue of ${:.2f}'.format(calculate_irr(train_data), calculate_nir(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permtest(df, n_trials = 10000):\n",
    "    \"\"\"\n",
    "    Compute a confidence interval for the irr and nir using a permutation method. \n",
    "    \n",
    "    Input parameters:\n",
    "        df: original dataframe on which to perform the test\n",
    "        alternative: type of test to perform, {'less', 'greater'}\n",
    "        n_trials: number of permutation trials to perform\n",
    "    \n",
    "    Output value:\n",
    "        a confidence interval containing 95% of values\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # initialize storage of permuted irr and nir\n",
    "    sample_irr = []\n",
    "    sample_nir = []\n",
    "    \n",
    "    # For each trial...\n",
    "    for _ in range(n_trials):\n",
    "        # randomly permute the grouping labels\n",
    "        df.Promotion = np.random.permutation(df.Promotion)\n",
    "        \n",
    "        # compute the irr and nir on the permuted dataset\n",
    "        irr = calculate_irr(df)\n",
    "        nir = calculate_nir(df)\n",
    "        \n",
    "        # and add the value to the list of samples\n",
    "        sample_irr.append(irr)\n",
    "        sample_nir.append(nir)\n",
    "    \n",
    "    irr_ci = []\n",
    "    irr_ci.append(np.percentile(np.array(sample_irr),0.025))\n",
    "    irr_ci.append(np.percentile(np.array(sample_irr),0.975))\n",
    "    nir_ci = []\n",
    "    nir_ci.append(np.percentile(np.array(sample_nir),0.025))\n",
    "    nir_ci.append(np.percentile(np.array(sample_nir),0.975))\n",
    "    \n",
    "    return irr_ci, nir_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0024224399938946196, -0.0018791527735942163],\n",
       " [-2224.6109999999994, -2098.3144999999995])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irr_ci, nir_ci = permtest(train_data, n_trials = 1000)\n",
    "irr_ci, nir_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.065691\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>purchase</td>     <th>  No. Observations:  </th>  <td> 84534</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 84525</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 30 Nov 2018</td> <th>  Pseudo R-squ.:     </th> <td>0.009692</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:09:58</td>     <th>  Log-Likelihood:    </th> <td> -5553.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -5607.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>7.070e-20</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Promotion</th> <td>    0.0473</td> <td>    0.062</td> <td>    0.758</td> <td> 0.449</td> <td>   -0.075</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>        <td>   -0.0516</td> <td>    0.036</td> <td>   -1.436</td> <td> 0.151</td> <td>   -0.122</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>        <td>   -0.0032</td> <td>    0.006</td> <td>   -0.515</td> <td> 0.606</td> <td>   -0.015</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>        <td>   -0.0588</td> <td>    0.031</td> <td>   -1.882</td> <td> 0.060</td> <td>   -0.120</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>        <td>    0.7350</td> <td>    0.080</td> <td>    9.175</td> <td> 0.000</td> <td>    0.578</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>        <td>    0.0833</td> <td>    0.037</td> <td>    2.238</td> <td> 0.025</td> <td>    0.010</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>        <td>   -0.0129</td> <td>    0.028</td> <td>   -0.461</td> <td> 0.645</td> <td>   -0.068</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>        <td>   -0.0201</td> <td>    0.068</td> <td>   -0.296</td> <td> 0.767</td> <td>   -0.153</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>     <td>   -5.6550</td> <td>    0.295</td> <td>  -19.178</td> <td> 0.000</td> <td>   -6.233</td> <td>   -5.077</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               purchase   No. Observations:                84534\n",
       "Model:                          Logit   Df Residuals:                    84525\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Fri, 30 Nov 2018   Pseudo R-squ.:                0.009692\n",
       "Time:                        13:09:58   Log-Likelihood:                -5553.1\n",
       "converged:                       True   LL-Null:                       -5607.4\n",
       "                                        LLR p-value:                 7.070e-20\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Promotion      0.0473      0.062      0.758      0.449      -0.075       0.170\n",
       "V1            -0.0516      0.036     -1.436      0.151      -0.122       0.019\n",
       "V2            -0.0032      0.006     -0.515      0.606      -0.015       0.009\n",
       "V3            -0.0588      0.031     -1.882      0.060      -0.120       0.002\n",
       "V4             0.7350      0.080      9.175      0.000       0.578       0.892\n",
       "V5             0.0833      0.037      2.238      0.025       0.010       0.156\n",
       "V6            -0.0129      0.028     -0.461      0.645      -0.068       0.042\n",
       "V7            -0.0201      0.068     -0.296      0.767      -0.153       0.113\n",
       "const         -5.6550      0.295    -19.178      0.000      -6.233      -5.077\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first create an intercept, map our Promotion variable to a numeric value and then fit our model\n",
    "train_data = add_constant(train_data)\n",
    "train_data.Promotion = train_data.Promotion.map({'Yes': 1, 'No': 0})\n",
    "model = sm.Logit(train_data.purchase, train_data[['Promotion', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'const']])\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our double-pronged analysis yields similar results:\n",
    "- fitting a logistic regression allows us to assess whether, controlling for other variables, receiving the promotion in itself has an impact on the probability of buying an object. Here, at the 10% significance level, we can conclude that there is a significant impact. We do not include interaction terms, for we are more interested in the overall impact of the promotion.\n",
    "- conducting a permutation test confirms this analysis on the two metrics of interest. With this test, we can use permutation methods to build a confidence interval for the NIR and IRR, and see that our observed values are outside of these intervals. Thus, we conclude there is a significant impact.\n",
    "\n",
    "Thus, the promotion seems to be beneficial when it comes to prompting a purchase. We now try to determine whether we can optimize our strategy by establishing rules around whom should receive the promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promotion Strategy Optimization\n",
    "\n",
    "Both metrics of interest boil down to increasing the response rate for people who do receive the promotions. In other words, we only want to send the promotion to people who are the most likely to purchase when they receive a promotion and avoid those for whom the promotion has no impact, since it costs money to send the promotion. The idea is to find values of the features associated with purchasing.\n",
    "\n",
    "This brings to mind a classification problem. We will build a decision tree on our training set, and use it to make predictions for the test set. These predictions, based on combinations of features, will help us decide who to send the promotion to, based on the probability of purchasing output by our model.\n",
    "\n",
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9875791092446915 but the F-1 score is 0.0\n"
     ]
    }
   ],
   "source": [
    "# We split our dataset into train and test sets and then fit a DecisionTreeClassifier\n",
    "# We first remove the constant that was created for our previous model\n",
    "train_data.drop(labels='const', axis=1, inplace=True)\n",
    "X = train_data.iloc[:,3:]\n",
    "y = train_data.purchase\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print('The accuracy is {} but the F-1 score is {}'.format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply fitting a decision tree is not enough: only about 1% of the customers in the training set purchased, regardless of them receiving the promotion or not. We have an imbalance in our classes, which biases our algorithm: a very high accuracy can be achieved by always predicting a customer will not purchase. We downsample our dominant class and re-train our model to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We downsample our dominant class in our training set\n",
    "i_class0 = np.where(y_train == 0)[0]\n",
    "i_class1 = np.where(y_train == 1)[0]\n",
    "\n",
    "# Number of observations in each class\n",
    "n_class0 = len(i_class0)\n",
    "n_class1 = len(i_class1)\n",
    "\n",
    "# For every observation of class 0, randomly sample from class 1 without replacement\n",
    "i_class0_downsampled = np.random.choice(i_class0, size=n_class1*2, replace=False)\n",
    "\n",
    "# Create new indices based on the downsampled class 0\n",
    "indices = np.concatenate((i_class0_downsampled, i_class1))\n",
    "np.random.shuffle(indices) # we shuffle to avoid order in our dataset\n",
    "\n",
    "# We subset our X_train and y_train datasets, we now have a ratio of 2:1 for class 0 vs 1\n",
    "X_train = X_train.iloc[indices]\n",
    "y_train = y_train.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9756313952800615 but the F-1 score is 0.0\n"
     ]
    }
   ],
   "source": [
    "# We can fit a new model\n",
    "tree = DecisionTreeClassifier(random_state=42, max_depth=4, min_samples_leaf=7, min_samples_split=20)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print('The accuracy is {} but the F-1 score is {}'.format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05217911, 0.23598669, 0.125073  , 0.34735228, 0.23940891,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now examine feature importance\n",
    "tree.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score of our decision tree is still extremely low, but we go ahead using it regardless to predict probability of purchasing. For our specific purposes, we will send a promotion to someone who, according to our model, has a probability of purchasing greater than 40%, as it is much higher than our baseline.\n",
    "\n",
    "### Promotion Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_features(row):\n",
    "    '''\n",
    "    A helper function to get the predicted probability of purchasing for a given individual and make a decision based on a\n",
    "    chosen boundary.\n",
    "    \n",
    "    INPUTS:\n",
    "    row: an array containing the values of interest in a row\n",
    "    \n",
    "    OUTPUT:\n",
    "    a string specifying whether a given individual (characterized by a row) should be sent the promotion or not\n",
    "    '''\n",
    "    if tree.predict_proba(row.values.reshape(1,-1))[0][1] > 0.4:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    '''\n",
    "    INPUT \n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an \n",
    "                   individual should recieve a promotion \n",
    "                   should be the length of df.shape[0]\n",
    "                \n",
    "    Ex:\n",
    "    INPUT: df\n",
    "    \n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "    \n",
    "    OUTPUT: promotion\n",
    "    \n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and \n",
    "    the last should not.\n",
    "    '''    \n",
    "    promotion = df.apply(compare_features, axis = 1)\n",
    "    \n",
    "    return promotion\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our promotion strategy, we test it out on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  See how well your strategy worked on our test data below!\n",
      "\n",
      "Your irr with this strategy is 0.0201.\n",
      "\n",
      "Your nir with this strategy is 311.65.\n",
      "We came up with a model with an irr of 0.0188 and an nir of 189.45 on the test set.\n",
      "\n",
      " How did you do?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02011544509583637, 311.6500000000001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will test your results, and provide you back some information \n",
    "# on how well your promotion_strategy will work in practice\n",
    "\n",
    "test_results(promotion_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our strategy outperforms the optimal strategy devised by Udacity. Our incremental response rate is 2.01% while our net incremental revenue is $311.65. This means that my strategy was more targeted (and thus more efficient, as more people responded), and reached a large enough audience to generate a lot of incremental revenue.\n",
    "\n",
    "Although this approach was devised with a decision tree, we could test different classification models (Random Forest, Logistic Regression, Gradient Boosting...) and see which yields the best results. We could also test various threshold (i.e. probability of purchasing) or try to remove outliers in the V2 feature. In order to test all these alternatives, the approach I would take would be to build a pipeline with GridSearch, and define a custom scoring method based on the NIR and IRR on the test set that the process would optimize towards, choosing the best combination of parameters (see the disaster_response_pipeline repository for an example). As a first iteration, the results are satisfactory here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
